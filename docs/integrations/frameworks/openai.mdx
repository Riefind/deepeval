---
id: openai
title: OpenAI
sidebar_label: OpenAI
---

## Quick Summary

DeepEval streamlines the process of evaluating and tracing your OpenAI applications through an **OpenAI client wrapper**, and supports both end-to-end and component-level evaluations.

## End-to-End Evaluation

To begin evaluating your OpenAI application, attach the `@observe` decorator to your LLM application, and replace your OpenAI client with DeepEval's OpenAI client.

```python showLineNumbers {2,19}
from deepeval.tracing import observe
from deepeval.openai import OpenAI

from deepeval.metrics import AnswerRelevancyMetric, BiasMetric
from deepeval.dataset import Golden
from deepeval import evaluate

client = OpenAI()
user_input = "Hello, how are you?"

client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": user_input}
    ],
    metrics=[AnswerRelevancyMetric(), BiasMetric()]
)
```

## Component-level Evaluation

DeepEval's OpenAI integration also supports component-level evaluations. As with end-to-end evaluation, simply add the `@observe` decorator to any OpenAI function component in your LLM application, and replace your existing OpenAI clients with DeepEval's OpenAI client.

```python showLineNumbers {2,25}
from deepeval.tracing import observe
from deepeval.openai import OpenAI

from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import Golden
from deepeval import evaluate

@observe(type="agent", metrics=[AnswerRelevancyMetric()])
def llm_app(input: str):

    # OpenAI integration
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": input}
        ],
        metrics=[BiasMetric()]
    )

    # Response
    return response.choices[0].message.content

evaluate(observed_callback=llm_app, goldens=[Golden("Hello, how are you?")])
```

## How it works

When you integrate DeepEvalâ€™s OpenAI client, DeepEval automatically:

- Populates span-level and trace-level `LLMTestCase`s with inputs, outputs, and tool calls from OpenAI
- Records span-level `LLMAttributes` including input, output, and token usage
- Logs hyperparameters such as model and system prompt for experiment analysis
- Converts `BaseSpan`s to `LlmSpan`s without the need to define a span type

### Evaluating Retrieval

Since retrieval context is not available through the OpenAI API, DeepEval **cannot** automatically populate the retrieval context field in the `LLMTestCase`.
:::info
To evaluate retrieval-based metrics like Contextual Relevancy, you'll need to manually populate and set the test case using `update_current_span`

```python showLineNumbers
...

client = OpenAI()
user_input = "Hello, how are you?"

client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": user_input}
    ],
    metrics=[AnswerRelevancyMetric(), BiasMetric()]
    # Optional test case parameters
    expected_output="Hello, how are you?",
    retrieval_context=["Document 1", "Document 2", "Document 3"]
    context="Document 1"
)
```

:::
